{"cells":[{"cell_type":"markdown","metadata":{"id":"ozsKKgROdb8_"},"source":["# The code here is the implementation of the Single Neuron Neural Network defined from: https://www.overleaf.com/project/63992e2c41d53fa75e5f7398\n"]},{"cell_type":"markdown","metadata":{"id":"bdzeQU7Wx2_z"},"source":["## Importing the packages"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"1X6x5wRkxwyU","executionInfo":{"status":"ok","timestamp":1672984577524,"user_tz":-480,"elapsed":2848,"user":{"displayName":"tiana chen","userId":"09015378256321236843"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","import pdb\n","from torch.autograd import grad"]},{"cell_type":"markdown","metadata":{"id":"wyJxHYlHyBGL"},"source":["## Defining the network\n","We define a single neuron neural network with initial weight $\\theta=0$. "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"SdZEaBfcyAT8","executionInfo":{"status":"ok","timestamp":1672984577524,"user_tz":-480,"elapsed":7,"user":{"displayName":"tiana chen","userId":"09015378256321236843"}}},"outputs":[],"source":["class SingleNet():\n","  def __init__(self):\n","    theta = torch.tensor([1.], requires_grad=True) # task specific initialization\n","    self.weight = theta # set initial weight to 1\n","  \n","  def __call__(self, x):\n","    return self.weight * x"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1672984577525,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"NilLPLUCfWs6","outputId":"91ec02d3-6117-4693-d24a-ecfac6d3948e"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([5.], grad_fn=<MulBackward0>)\n"]}],"source":["## TEST ## \n","# we test if the neuron works x=5 f_w=w*x=1*5=5\n","net = SingleNet()\n","print(net(5))"]},{"cell_type":"markdown","metadata":{"id":"laXNz3usy3Qm"},"source":["Defining dataset\n","\n","---\n","\n","Tables:\n","```\n","D_1 | x   y  |$f_\\theta$ \n","----|------------------\n","Q 1 | 1   2  |- \n","S 1 | 2   4  |2 \n","    | 3   1  |3 \n","\n","```\n","\n","```\n","D_2 | x   y  |$f_\\theta$ \n","----|------------------\n","Q 2 | 4   1  |- \n","S 2 | 5   3  |5 \n","    | 6   0  |6 \n","```"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672984577525,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"RZnCPixKdznP","outputId":"243fb520-2653-413f-c1c7-abb26604cd80"},"outputs":[{"output_type":"stream","name":"stdout","text":["x_D1 (support set): tensor([2, 3])\n","y_D1 (support set): tensor([4, 1])\n"]}],"source":["# We define 2 datasets for our case. \n","D1 = {'query': torch.tensor([(1, 2)], dtype=torch.int64), 'support': torch.tensor([(2, 4), (3, 1)])} # (x, y) pairs for query (Q1) and support (S1) set.\n","D2 = {'query': torch.tensor([(4, 1)], dtype=torch.int64), 'support': torch.tensor([(5, 3), (6, 0)])}\n","D_all = [D1, D2]\n","print(f\"x_D1 (support set): {D1['support'][:,0]}\")  #x\n","print(f\"y_D1 (support set): {D1['support'][:,1]}\")  # y\n"]},{"cell_type":"markdown","metadata":{"id":"bZBYTdOh0KlU"},"source":["We define the loss function (MSE loss)\n","\n","$\\mathcal{L}_{S_j}(f_\\theta(x),y)=\\sum_{(x_i^j,y_i^j) \\in S_j}{(y_i^j-\\theta x_i^j)^2}$"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"8csklaGmyjZz","executionInfo":{"status":"ok","timestamp":1672984577525,"user_tz":-480,"elapsed":4,"user":{"displayName":"tiana chen","userId":"09015378256321236843"}}},"outputs":[],"source":["def loss(weight, dataset, mode='train'):\n","  '''\n","  Regression loss over dataset\n","  '''\n","  # l1 loss\n","  if mode == 'train':\n","    data = dataset['support']\n","  if mode == 'test':\n","    data = dataset['query']\n","  return torch.sum((data[:, 1] - weight * data[:, 0])**2)"]},{"cell_type":"markdown","metadata":{"id":"Ne-4Jfg90trj"},"source":["Then we define the gradient of loss function\n","\n","$\\frac{\\partial \\mathcal{L}_{S_j}(f_\\theta)}{\\partial \\theta}=-2\\sum_{(x_i^j,y_i^j)\\in S_j}{x_i^j(y_i^j-\\theta x_i^j)}$"]},{"cell_type":"markdown","metadata":{"id":"Yp5BLlmw4Cgo"},"source":["Task-specific weight (inner) update is calculated\n","\n","$\\varphi_j(\\theta^{(0)}) = \\theta^{(0)} -\\alpha \t\\frac{\\partial \\mathcal{L}_{S_j}(f_\\theta)}{\\partial w} \\Bigg|_{\\theta=\\theta^{(0)}}$\n","\n","$\n","  = \\theta^{(0)}+2\\alpha \\sum_{(x_i^j,y_i^j)\\in S_j}{x_i^j(y_i^j-\\theta^{(0)}x_i^j)}  $\n","\n","\n","where $\\alpha=0.1$ is the task-specific learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8pSC_7ImB7i"},"outputs":[],"source":["def inner_gradient(net, dataset, mode='train', weight=0, compute_autograd=False):\n","  if mode == 'train':\n","    data = dataset['support']\n","    weight = net.weight\n","\n","  if mode == 'test':\n","    data = dataset['query']\n","    weight = net.weight\n","    print(f\"task-specific weight: {weight}\")\n","  if compute_autograd:\n","    task_specific_loss = loss(weight, dataset, mode=mode)\n","\n","    # using backward here\n","    # task_specific_loss.backward()\n","    # task_specific_gradients = net.weight.grad\n","\n","    # instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n","    task_specific_gradients = grad(loss, net.weight, create_graph=True)\n","    print(f\"First order loss grad: {task_specific_gradients}\")  # need to be converted into orch.tensor\n","  else: \n","    # compute gradient manually using formula\n","    task_specific_gradients = -2 * torch.sum(data[:, 0] * (data[:, 1] - weight.item() * data[:, 0]))\n","  return task_specific_gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-J1HNcB4ybw"},"outputs":[],"source":["def inner_weight(net, dataset, mode='train', alpha=0.1, compute_autograd=True):\n","  '''\n","  Compute task-specific (inner) weights on the support set\n","  param net: meta model weight\n","  param dataset: task\n","  param alpha: task-specific learning rate\n","  '''\n","  loss_grad = inner_gradient(net, dataset, mode=mode, compute_autograd=compute_autograd)\n","  task_specific_weight = net.weight - alpha * torch.tensor(loss_grad)\n","  return task_specific_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1672913489342,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"j1Groc6v6CDk","outputId":"67f399ba-7678-459e-e411-02938a60da07"},"outputs":[{"output_type":"stream","name":"stdout","text":["First order loss grad: tensor([102.])\n","task sepcific weight (w'1) of D1: tensor([-9.2000], grad_fn=<SubBackward0>)\n","First order loss grad: tensor([194.])\n","task sepcific weight (w'2) of D2: tensor([-18.4000], grad_fn=<SubBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-52-3d139afb2476>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  task_specific_weight = net.weight - alpha * torch.tensor(loss_grad)\n"]}],"source":["w1 = inner_weight(net, D1)\n","print(f\"task sepcific weight (w'1) of D1: {w1}\")\n","\n","w2 = inner_weight(net, D2)\n","print(f\"task sepcific weight (w'2) of D2: {w2}\")"]},{"cell_type":"markdown","metadata":{"id":"9iicVm9J7y5E"},"source":["The meta gradient is calculated based on summation of gradient of loss over query set of each task: \n","\n","$\\sum_{j} \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})$\n","\n","which can be calculated using chain rule:\n","\n","$\\frac{\\partial \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})}{\\partial \\theta^{(0)}} = \\underbrace{\\frac{\\partial \\varphi_j(\\theta^{(0)})}{\\partial \\theta^{(0)}}}_{\\circ}\\underbrace{\\frac{\\partial \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})}{\\partial \\varphi_j(\\theta^{(0)})}}_{\\star}$\n","\n","$\\frac{\\partial \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})}{\\partial \\theta^{(0)}} = \\underbrace{\\left(1-2\\alpha\\sum_{(x_q^j, y_q^j)\\in Q_j}{\\left( x_q^j\\right)^2}\\right)}_{eq. 14}\\underbrace{\\left(-2\\sum_{(x_q^j, y_q^j)\\in Q_j}{x_q^j(y_q^j-{\\varphi_j(\\theta^{(0)})}x_q^j)}\\right)}_{eq. 13}$"]},{"cell_type":"code","source":[],"metadata":{"id":"6XRbBEMGeSML"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwRV1BwB5i0s"},"outputs":[],"source":["def meta_gradient(net, dataset, weight, mode='test', alpha=0.1, compute_autograd=False):\n","  '''\n","  Manual calculation of gradient (NO COMPUTE_AUTOGRAD PARAM)\n","  eqn 10\n","  '''\n","  if mode == 'train':\n","    data = dataset['support']\n","  if mode == 'test':\n","    data = dataset['query']\n","  # eqn 8\n","  \"\"\"if compute_autograd == True:\n","    #use_autograd = True\n","    eq8 = inner_weight(net, dataset, alpha=0.1, compute_autograd = True )\"\"\"\n","\n","  # use inner_gradient to handle everything.\n","  loss_grad = inner_gradient(net, dataset, mode=mode, weight=weight,compute_autograd=compute_autograd)\n","    \n","  if compute_autograd == True: #TC\n","    #pdb.set_trace()\n","    loss_grad = inner_gradient(net, dataset, mode=mode, weight=weight,compute_autograd=True)\n","    print(f\"{loss_grad = }\")\n","\n","    eq8 = torch.tensor(loss_grad)\n","    eq9 = torch.tensor([1]) - 2 * alpha * torch.tensor(data[:, 0]**2)\n","\n","  else: #TC\n","    eq8 = inner_gradient(net, dataset, mode=mode, weight=weight)\n","    print(f\"Eq: 13: {eq8:}\")\n","    # eqn 9\n","    eq9 = torch.tensor([1]) - 2 * alpha * torch.sum(data[:, 0]**2)\n","    print(f\"Eq: 14: {eq9:}\")\n","\n","  new_meta_gradient = eq8 * eq9\n","\n","  return new_meta_gradient"]},{"cell_type":"markdown","metadata":{"id":"FfPr7iUE9EnE"},"source":["So the meta gradients for task 1 $D_1$ and task 2 $D_2$ can be calculated as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1672909276441,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"1dK1s_219OwU","outputId":"ec461689-1e7c-4a1b-c76e-115f1238a8fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["task-specific weight: tensor([0.6000], grad_fn=<SubBackward0>)\n","Eq: 13: -2.799999952316284\n","Eq: 14: tensor([0.8000])\n","Meta gradient for D1 (query set): tensor([-2.2400])\n","task-specific weight: tensor([-8.2000], grad_fn=<SubBackward0>)\n","Eq: 13: -270.3999938964844\n","Eq: 14: tensor([-2.2000])\n","Meta gradient for D2 (query set): tensor([594.8800])\n"]}],"source":["print(f\"Meta gradient for D1 (query set): {meta_gradient(net, D1, w1)}\")\n","print(f\"Meta gradient for D2 (query set): {meta_gradient(net, D2, w2)}\")"]},{"cell_type":"markdown","metadata":{"id":"mz0utXc14-fw"},"source":["The meta weight updates\n","\n","$\\theta^{(1)} =\\tilde{\\theta}^{(0)}-\\beta \\frac{\\partial}{\\partial \\theta^{(0)}} \\sum_{j} \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})\\Bigg|_{\\theta^{(0)}=\\tilde{\\theta}^{(0)}}$\n","\n","$= \\tilde{\\theta}^{(0)}-\\beta \\sum_{j} \\underbrace{\\frac{\\partial \\mathcal{L}_{Q_j}(f_{\\varphi_j(\\theta^{(0)})})}{\\partial \\theta^{(0)}}}_{*}\\Bigg|_{\\theta^{(0)}=\\tilde{\\theta}^{(0)}}$\n","\n","where $\\beta=0.5$ is meta learning weight, and $\\tilde{\\theta}^{(0)}=1$"]},{"cell_type":"code","source":["net = SingleNet()"],"metadata":{"id":"HSQKKQj6eVH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-Sqg_NIgXry"},"outputs":[],"source":["def meta_weight(net, datasets, weight, beta=0.5, compute_autograd=True): #TC: only change compute_autograd variable here\n","  '''\n","  Compute meta (outer) weights on the query set\n","  param weight: weight vector of task specific weights\n","  '''\n","  # eqn 6\n","  # update weights\n","  # all_query_sets = list(map(lambda x: x['query'], datasets))\n","  # print(f'{all_query_sets=}')\n","  if compute_autograd:\n","    w1 = inner_weight(net, datasets[0], mode='train')\n","    # w2 = inner_weight(net, datasets[1], mode='train')\n","    print(f'{w1=}')\n","    # loss_1 = loss(w1, datasets[0], mode='test')\n","    loss_1 = loss(w1, datasets[0], mode='test')\n","    # loss_2 = loss(w2, datasets[1], mode='test')\n","\n","    print(f'{loss_1=}')\n","    loss_1.backward()\n","    \n","\n","    print(net.weight.grad)\n","  else:\n","    # MANUAL CALCULATION USING META GRADIENT\n","    all_meta_gradient = torch.tensor(list((map(lambda i: meta_gradient(net, datasets[i], weight=weight[i], mode='test'), list(range(len(datasets)))))))\n","    print(f\"Gradients of loss over query sets: {all_meta_gradient}\")\n","    net.weight = net.weight - (beta * torch.sum(all_meta_gradient))\n","    print(f\"Updated meta model weight after one gradient step: {net.weight}\")"]},{"cell_type":"code","source":["meta_weight(net, D_all, [w1, w2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KH0rI8LVawNa","executionInfo":{"status":"ok","timestamp":1672914788305,"user_tz":-480,"elapsed":7,"user":{"displayName":"tiana chen","userId":"09015378256321236843"}},"outputId":"517a92db-796c-44c1-a5e0-e72def03ece1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["First order loss grad: tensor([4.])\n","w1=tensor([0.6000], grad_fn=<SubBackward0>)\n","First order loss grad: tensor([8.])\n","loss_1=tensor(3.2400, grad_fn=<SumBackward0>)\n","tensor([4.4000])\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-52-3d139afb2476>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  task_specific_weight = net.weight - alpha * torch.tensor(loss_grad)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1672909483085,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"xiEJKhLUGC5C","outputId":"d3826583-dd9b-4641-cdc6-67e3c1a9d56b"},"outputs":[{"output_type":"stream","name":"stdout","text":["task-specific weight: tensor([0.6000], grad_fn=<SubBackward0>)\n","Eq: 13: -2.799999952316284\n","Eq: 14: tensor([0.8000])\n","task-specific weight: tensor([-8.2000], grad_fn=<SubBackward0>)\n","Eq: 13: -270.3999938964844\n","Eq: 14: tensor([-2.2000])\n","Gradients of loss over query sets: tensor([ -2.2400, 594.8800])\n","Updated meta model weight after one gradient step: tensor([-295.3200], grad_fn=<SubBackward0>)\n"]}],"source":["meta_weight(net, D_all, [w1, w2])"]},{"cell_type":"markdown","metadata":{"id":"FLynNaA5HTOr"},"source":["## Autograd: Single Neuron Neural Netwrok model weight update using autograd\n","\n","The followings show the MAML model using autograd (automatic grdaient calculation)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1672909530698,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"gGBeAMvRXKf2","outputId":"a1e2aac1-9b2e-46d5-bcc3-2cdea4d4b501"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.], requires_grad=True)"]},"metadata":{},"execution_count":14}],"source":["# we define a new net model \n","net2 = SingleNet()\n","net2.weight"]},{"cell_type":"markdown","metadata":{"id":"fxN0MSEqYXMy"},"source":["Compute task-specific weight using autograd\n","\n","* Compute the loss using `loss()` function $$\\mathcal{L}_{S_j}(f_\\theta(x),y)=\\sum_{(x_i^j,y_i^j) \\in S_j}{(y_i^j-\\theta x_i^j)^2}$$\n","* Compute gradient of loss using autograd: $$\\frac{\\partial \\mathcal{L}_{S_j}(f_\\theta)}{\\partial \\theta}=-2\\sum_{(x_i^j,y_i^j)\\in S_j}{x_i^j(y_i^j-\\theta x_i^j)}$$\n","* Calculate the task-specific weights ($\\varphi_j$)\n","\n","Initial network weight $\\theta^{(0)}=1$"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":279,"status":"ok","timestamp":1672909533637,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"lhu9JbD-rkfZ","outputId":"0741712d-00d4-4b53-d39a-e1d18f5d8a17"},"outputs":[{"output_type":"stream","name":"stdout","text":["First order loss grad: (tensor([4.], grad_fn=<SumBackward1>),)\n","Gradient of loss for D1 (autograd): (tensor([4.], grad_fn=<SumBackward1>),)\n","First order loss grad: (tensor([92.], grad_fn=<SumBackward1>),)\n","Gradient of loss for D2 (autograd): (tensor([92.], grad_fn=<SumBackward1>),)\n"]}],"source":["print(f\"Gradient of loss for D1 (autograd): {inner_gradient(net2, D1,compute_autograd = True)}\")\n","print(f\"Gradient of loss for D2 (autograd): {inner_gradient(net2, D2,compute_autograd= True)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270,"status":"ok","timestamp":1672909572741,"user":{"displayName":"tiana chen","userId":"09015378256321236843"},"user_tz":-480},"id":"vVmBmXLNsJ4o","outputId":"47698159-425c-4481-d9c0-e3870b5cfa06"},"outputs":[{"output_type":"stream","name":"stdout","text":["First order loss grad: (tensor([4.], grad_fn=<SumBackward1>),)\n","w1 =tensor([0.6000], grad_fn=<SubBackward0>)\n","First order loss grad: (tensor([92.], grad_fn=<SumBackward1>),)\n","w2 =tensor([-8.2000], grad_fn=<SubBackward0>)\n"]}],"source":["#ww=net2.weight.detach()\n","w1=inner_weight(net2, D1, alpha=0.1, compute_autograd = True)\n","print(f\"{w1 =}\")\n","\n","w2=inner_weight(net2, D2, alpha=0.1, compute_autograd = True)\n","print(f\"{w2 =}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1672891585634,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"-oOTxGzF_t_2","outputId":"fa959d4c-1d03-4da3-e34e-e96e69cd7302"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-243-ba6cfd775771>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tw1= torch.tensor(w1, requires_grad = True)\n","<ipython-input-243-ba6cfd775771>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  tw2 =torch.tensor(w2, requires_grad = True)\n"]}],"source":["tw1 = torch.tensor(w1, requires_grad = True)\n","tw2 = torch.tensor(w2, requires_grad = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":345},"executionInfo":{"elapsed":1348,"status":"error","timestamp":1672891604683,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"iUH9W5mosVkB","outputId":"a95bbabc-0adc-48ee-e08b-493710fe4f9d"},"outputs":[{"name":"stdout","output_type":"stream","text":["task-specific weight: tensor([0.6000], grad_fn=<SubBackward0>)\n","First order loss grad: (tensor([-2.], grad_fn=<MulBackward0>),)\n","loss_grad = (tensor([-2.], grad_fn=<MulBackward0>),)\n","Meta gradient for D1 (query set) using autograd: tensor([-1.6000])\n","task-specific weight: tensor([-8.2000], grad_fn=<SubBackward0>)\n","First order loss grad: (tensor([24.], grad_fn=<MulBackward0>),)\n","loss_grad = (tensor([24.], grad_fn=<MulBackward0>),)\n","Meta gradient for D2 (query set) using autograd: tensor([-52.8000])\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-256-e648d4262dfe>:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  eq9 = torch.tensor([1]) - 2 * alpha * torch.tensor(data[:, 0]**2)\n"]}],"source":["print(f\"Meta gradient for D1 (query set) using autograd: {meta_gradient(net2, D1, tw1, compute_autograd = True)}\")\n","print(f\"Meta gradient for D2 (query set) using autograd: {meta_gradient(net2, D2, tw2, compute_autograd = True)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1672881773797,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"friBqe93dLo2","outputId":"1cde6aef-dd70-4407-8845-4a3e26253619"},"outputs":[{"data":{"text/plain":["(tensor([92.], grad_fn=<SumBackward1>),)"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["net2(4)\n","\n","x = D2['support'][:,0]\n","y = D2['support'][:,1]\n","new_loss = torch.sum((y - net2(x))**2)\n","\n","# instead of using loss.backward(), use torch.autograd.grad() to compute gradients\n","loss_grads = grad(new_loss, net2.weight, create_graph=True)\n","loss_grads"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1672881218251,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"iBAdopL7eceu","outputId":"a1086e8c-a806-430a-a1be-cd059a0bf506"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1.], requires_grad=True) (tensor([32.], grad_fn=<MulBackward0>),)\n"]}],"source":["d2loss =[]\n","\n","drv = grad(loss_grads, net2.weight, create_graph=True)\n","d2loss.append(drv)\n","print(net2.weight, drv)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":369,"status":"ok","timestamp":1672881797846,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"-6vk6xfQFg2G","outputId":"7d16948c-3f0a-41b1-ca41-71e4278dcb0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["loss_grads=(tensor([4.], grad_fn=<SumBackward1>),)\n","w1 =tensor([0.6000], grad_fn=<SubBackward0>)\n","loss_grads=(tensor([92.], grad_fn=<SumBackward1>),)\n","w2 =tensor([-8.2000], grad_fn=<SubBackward0>)\n"]}],"source":["#ww=net2.weight.detach()\n","w1=inner_weight2(net2, D1, alpha=0.1, compute_autograd = True)\n","print(f\"{w1 =}\")\n","\n","w2=inner_weight2(net2, D2, alpha=0.1, compute_autograd = True)\n","print(f\"{w2 =}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7u0aeJ0gY-hE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1672829107528,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"OCpxfS34kxzk","outputId":"a9ce5e1f-99e7-40b7-e410-692876c70d17"},"outputs":[{"name":"stdout","output_type":"stream","text":["weight.grad=tensor([4.])\n","weight.grad=tensor([96.])\n"]}],"source":["weight=net2.weight\n","grad_s1= compute_first_order_autograd(weight, losses_s[0])\n","weight=net2.weight   #  question: how to reuse the initialization weight in pytorch\n","grad_s2= compute_first_order_autograd(weight, losses_s[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J5_RfPWfj25Q"},"outputs":[],"source":["alpha = 0.1\n","task_specific_weight1 = net2.weight - alpha * grad_s1\n","task_specific_weight2 = net2.weight - alpha * grad_s2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1672817690802,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"FCYnsRE0omKA","outputId":"4e91b04d-67f0-4627-b592-87904774a6fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["task_specific_weight1 = tensor([-8.6000], grad_fn=<SubBackward0>)\n","task_specific_weight2 = tensor([-8.6000], grad_fn=<SubBackward0>)\n"]},{"data":{"text/plain":["tensor([1.], requires_grad=True)"]},"execution_count":163,"metadata":{},"output_type":"execute_result"}],"source":["print(f\"{task_specific_weight1 = }\")\n","print(f\"{task_specific_weight2 = }\")\n","net2.weight"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":345,"status":"ok","timestamp":1672817602670,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"fN8_Hye2qmWR","outputId":"5b92d0be-cf36-4977-f7cf-ce6d8ccdc3de"},"outputs":[{"data":{"text/plain":["tensor([96.])"]},"execution_count":159,"metadata":{},"output_type":"execute_result"}],"source":["net2(D1['support'][:,0])\n","grad_s1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":355,"status":"ok","timestamp":1672813229286,"user":{"displayName":"Somayeh Ebrahimkhani","userId":"03078760878635932867"},"user_tz":-480},"id":"B6C7TgGuH71W","outputId":"edb57923-a936-4043-b678-66ce38f4084a"},"outputs":[{"name":"stdout","output_type":"stream","text":["net2.weight=tensor([1.], requires_grad=True)\n","None\n","rloss=tensor(8., grad_fn=<SumBackward0>)\n","net2.weight.grad=tensor([4.])\n"]}],"source":["print(f\"{net2.weight=}\")\n","print(f\"{net2.weight.grad}\")\n","rloss = loss(net2.weight,D1)\n","print(f\"{rloss=}\")\n","rloss.backward()\n","print(f\"{net2.weight.grad=}\")"]},{"cell_type":"code","source":["https://github.com/GauravIyer/MAML-Pytorch/blob/master/Experiment%201/Experiment_1_Sine_Regression.ipynb"],"metadata":{"id":"G1aAT3I04GGv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}